{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Goal**\n\nThis is a covid-19 analysis notebook\n\nWe are trying to predict # num of covid-19 patients from each states for a 2-week date range.\n\nAlgorithim used: Multi step LSTM using univariate features \n\nReference :https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/\n\n\nThere are three assumption that we are making here \nAssumption 1: We are assuming following to be demographics of India \n\n25 % - very young 0 - 18 \n\n45% - youth  \n\n30% - senior citizen (55+) \n\nWe are observing over a period of time -65% of total predicted patients  will require hospitalization\n\n\nAssumption 2: out of total beds availale in state only 5% of beds are available for corona patients \n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"covid_data = pd.read_csv(\"/kaggle/input/covid19-in-india/covid_19_india.csv\")\nstate_test_data = pd.read_csv(\"/kaggle/input/covid19-in-india/StatewiseTestingDetails.csv\")\nindividuals_details_data = pd.read_csv(\"/kaggle/input/covid19-in-india/IndividualDetails.csv\")\npopulation_india = pd.read_csv(\"/kaggle/input/covid19-in-india/population_india_census2011.csv\")\nhospital_beds=pd.read_csv(\"/kaggle/input/covid19-in-india/HospitalBedsIndia.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" covid_19_india data\")\ncovid_data.head()\nprint(\"StatewiseTestingDetails\")\nprint(state_test_data.head())\nprint(\"population_india_census2011\")\nprint(population_india.head())\n#print(covid_data.columns)\ncovid_data_grouped=covid_data[covid_data['Date']==covid_data['Date'].max()]\ncovid_data_grouped=covid_data_grouped.loc[:,['State/UnionTerritory','Cured','Deaths', 'Confirmed']]\ncovid_data_grouped.columns=['State','Cured','Deaths', 'Confirmed']\ncovid_data_grouped.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Travel notes from air passangers\")\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = individuals_details_data.notes.values\nwordcloud = WordCloud(\n    width = 100,\n    height = 50,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\n\nfig = plt.figure(\n    figsize = (10, 5),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_deaths=covid_data_grouped.sort_values(by='Confirmed',ascending=False)[1:5]\nimport seaborn as sns\nplt.figure(figsize= (5,4))\nplt.xticks(fontsize = 10)\nplt.yticks(fontsize = 10)\nplt.xlabel(\"Confirmed cases\",fontsize = 30)\nplt.ylabel('State',fontsize = 30)\nplt.title(\"Top 5 states having most Confirmed cases\" , fontsize = 30)\nax = sns.barplot(x = top_deaths.Confirmed, y = top_deaths.State)\nfor i, (value, name) in enumerate(zip(top_deaths.Confirmed,top_deaths.State)):\n    ax.text(value, i-.05, f'{value:,.0f}',  size=10, ha='left',  va='center')\nax.set(xlabel='Total cases', ylabel='State/UT')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_deaths=covid_data_grouped.sort_values(by='Deaths',ascending=False)[0:5]\nimport seaborn as sns\nplt.figure(figsize= (5,4))\nplt.xticks(fontsize = 10)\nplt.yticks(fontsize = 10)\nplt.xlabel(\"Confirmed cases\",fontsize = 30)\nplt.ylabel('State',fontsize = 30)\nplt.title(\"Top 5 states having most Deaths\" , fontsize = 30)\nax = sns.barplot(x = top_deaths.Deaths, y = top_deaths.State)\nfor i, (value, name) in enumerate(zip(top_deaths.Deaths,top_deaths.State)):\n    ax.text(value, i-.05, f'{value:,.0f}',  size=10, ha='left',  va='center')\nax.set(xlabel='Total cases', ylabel='State/UT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DEEP DIVE ANALYSIS TO PREDICT # OF POSITIVE CASES FOR TWO WEEK BASED ON LAST FEW WEEK DATA** ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(population_india.columns)\n#print(state_data_distibution.columns)\nprint(covid_data_grouped.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading containmnet zones data  found from other source","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"containment_zones = pd.read_excel(\"/kaggle/input/containment-zones-new/containment_zones.csv.xlsx\")\ncontainment_zones.head()\ncontainment_zones_grouped=containment_zones.loc[containment_zones['Classification']=='Red Zone'].groupby('State')['Classification'].count().reset_index()\ncontainment_zones_grouped.columns=['State','num_containment_zones']\ncontainment_zones_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preparing dataset for Univariate LSTM Analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merging all dataset\n#covid_data_distribution=pd.merge(pd.merge(covid_data,state_test_data,how='right', \\\n#                                 left_on=['State/UnionTerritory','Date'] ,right_on=['State','Date']), \\\n#                                 population_india, \\\n#                                 how='left', right_on='State / Union Territory',left_on='State/UnionTerritory')\n\n\ncovid_data_distribution=pd.merge(state_test_data,population_india, how='left',left_on=['State'],right_on='State / Union Territory')\n                                \n#print(covid_data_distribution.shape)\ncovid_data_distribution['State']=covid_data_distribution['State'].str.strip()\n#print(covid_data_distribution.isnull().sum())\n#Merging containmnet _zone data \n\ncovid_data_distribution=pd.merge(covid_data_distribution,containment_zones_grouped[['num_containment_zones','State']],\n                         on='State',how='left')\ncovid_data_distribution['PopulationDensity']=covid_data_distribution['Density'].str.replace(',',\"\").str.split('/km2').str[0].astype(float)\ncovid_data_distribution.fillna(covid_data_distribution['PopulationDensity'].mean(),inplace=True)\ncovid_data_distribution['PopulationDensity'].unique()\n\ncovid_data_distribution['weighted_density']=np.log(covid_data_distribution['PopulationDensity'])*covid_data_distribution['num_containment_zones']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model-Univariate LSTM\nFeatures used : Total positive cases \nTimesteps used :5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom math import ceil\nfrom datetime import datetime\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend\nfrom tensorflow.keras.models import Sequential,load_model\nfrom tensorflow.keras.layers import Dense,LSTM,Dropout,Flatten\nfrom tensorflow.keras import optimizers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#By looking at data we realise Density and num_containmnet_zones will be same throughout for each state .\nThat is why going ahead only with num of positive patient as a feature for univariate LSTM Model\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(covid_data_distribution.columns)\n#Retaining columns required for model\n\ndf=covid_data_distribution.loc[:,['Date','State','Positive']]\ndf.sort_values(by=['State','Date'],inplace=True)\ndf.isnull().sum()\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will prepare data for training and validation as required by LSTM layer for input .\nHere for each state covid-19 patients is being predicted by looking at its past data for 10-days.\nSince hospitals/ authorities need to know a few time step ahead about requiremnet , we are predicting output for next 7 days ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling missing values \nfinal_df_all=pd.DataFrame()\n\nfor state in df.State.unique():\n    #print(chosen_ip) \n    chosen_state_data=df[df['State']==state].sort_values('Date')\n    \n    chosen_state_data['Date']=pd.to_datetime(chosen_state_data['Date'],format='%Y-%m-%d') \n    dataset_all_date=pd.DataFrame(pd.date_range(start='2020-04-01',end='2020-05-31',freq='D'),\n                                        columns=['DailyDate'])\n    final_df = dataset_all_date.merge(chosen_state_data,\n                                        how='left',\n                                            left_on='DailyDate',\n                                            right_on='Date')\n\n    #print(final_df['Positive'].isnull().sum())\n    final_df['Positive'].ffill(axis='rows',inplace=True)\n    final_df['Positive'].fillna(0,inplace=True)\n    final_df['State'].fillna(state,inplace=True)\n    #print(final_df['Positive'].isnull().sum()) \n    final_df.drop(labels='Date',axis=1,inplace=True)\n    #print(final_df.count())\n    final_df_all=final_df_all.append(final_df)\n    \n#CReating timesteps for univariate LSTM\nx_train_df=[]\ny_train_df=[]\nmax_all=[]\nmin_all=[]\nsc=MinMaxScaler()\nn_steps=10\nfuture_days=7\nfor State in final_df_all['State'].unique():\n    final_df=final_df_all[final_df_all['State']==State]\n    final_df.sort_values(by='DailyDate',ascending=True)\n    #for i in range(n_steps,len(final_df)-1):\n    for i in range(n_steps,len(final_df)-future_days-n_steps):\n        print(i,i-n_steps,i+future_days-1)\n        #print(final_df.Positive.iloc[i+future_days-1:i+future_days].values.reshape(-1,1))\n        x_train_steps=final_df.Positive.iloc[i-n_steps:i].values.reshape(-1,1)\n        y_train_steps=final_df.Positive.iloc[i+future_days-1:i+future_days].values.reshape(-1,1)\n        #print(y_train_steps)\n        x_train_scaled=sc.fit_transform(x_train_steps)\n        y_train_scaled=sc.transform(y_train_steps)\n        x_train_df.append(x_train_scaled)\n        y_train_df.append(y_train_scaled)\n        max=sc.data_max_\n        min=sc.data_min_\n        max_all.append(max)\n        min_all.append(min)\nx_train=np.array(x_train_df)\ny_train=np.array(y_train_df)\nmax_all_train=np.array(max_all)\nmin_all_train=np.array(min_all)\nprint(x_train.shape,y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preparing validation data\nfinal_df_all=pd.DataFrame()\n\nfor state in df.State.unique():\n    #print(chosen_ip) \n    chosen_state_data=df[df['State']==state].sort_values('Date')\n    \n    chosen_state_data['Date']=pd.to_datetime(chosen_state_data['Date'],format='%Y-%m-%d') \n    dataset_all_date=pd.DataFrame(pd.date_range(start='2020-05-10',end='2020-06-04',freq='D'),\n                                        columns=['DailyDate'])\n    final_df = dataset_all_date.merge(chosen_state_data,\n                                        how='left',\n                                            left_on='DailyDate',\n                                            right_on='Date')\n\n    #print(final_df['Positive'].isnull().sum())\n    final_df['Positive'].ffill(axis='rows',inplace=True)\n    final_df['Positive'].fillna(0,inplace=True)\n    final_df['State'].fillna(state,inplace=True)\n    #print(final_df['Positive'].isnull().sum()) \n    final_df.drop(labels='Date',axis=1,inplace=True)\n    #print(final_df.count())\n    final_df_all=final_df_all.append(final_df)\n#print(final_df_all.shape)    \n#CReating timesteps for univariate LSTM\nx_test_df=[]\ny_test_df=[]\ny_state_test_df=[]\ny_date_df=[]\nmax_all=[]\nmin_all=[]\ny_test_unscaled=[]\nsc=MinMaxScaler()\nn_steps=10\nfuture_days=7\n\nfor State in final_df_all['State'].unique():\n    final_df=final_df_all[final_df_all['State']==State]\n    final_df.sort_values(by='DailyDate',ascending=True)\n    #print(final_df.shape)\n    for i in range(n_steps,len(final_df)-future_days):\n        print(i,i-n_steps,i+future_days-1)\n        x_test_steps=final_df.Positive.iloc[i-n_steps:i].values.reshape(-1,1)\n        #print(final_df.Positive.iloc[i-n_steps:i].values.reshape(-1,1))\n        y_test_steps=final_df.Positive.iloc[i+future_days-1:i+future_days].values.reshape(-1,1)\n        #print(x_test_steps.shape)\n        x_test_scaled=sc.fit_transform(x_test_steps)\n        y_test_scaled=sc.transform(y_test_steps)\n        x_test_df.append(x_test_scaled)\n        y_test_df.append(y_test_scaled)\n        y_state=final_df['State'].unique()\n        y_date=final_df.DailyDate.iloc[i+future_days-1:i+future_days].unique()\n        y_test_steps=final_df.Positive.iloc[i+future_days-1:i+future_days].unique()\n        y_test_unscaled.append(y_test_steps)\n        y_state_test_df.append(y_state)\n        y_date_df.append(y_date)\n        max=sc.data_max_\n        min=sc.data_min_\n        max_all.append(max)\n        min_all.append(min)\nx_test=np.array(x_test_df)\ny_test=np.array(y_test_df)\ny_state_test=np.array(y_state_test_df)\ny_date=np.array(y_date_df)\ny_test_unscaled_all=np.array(y_test_unscaled)\nmax_all_test=np.array(max_all)\nmin_all_test=np.array(min_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking shape of data -is it fit for LSTM input","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_state_test.shape)\nprint(y_test.shape)\nprint(y_date.shape)\nprint(y_test_unscaled_all.shape)\nprint(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\nprint(x_train.shape ,x_test.shape)\ny_train=y_train.reshape(x_train.shape[0])\ny_test=y_test.reshape(y_test.shape[0])\ny_state_test=y_state_test.reshape(y_state_test.shape[0])\ny_date=y_date.reshape(y_date.shape[0])\ny_test_unscaled=y_test_unscaled_all.reshape(y_test_unscaled_all.shape[0])\nprint(y_train.shape,y_test.shape,y_state_test.shape,y_date.shape,y_test_unscaled.shape)\nmax_all_test=max_all_test.reshape(max_all_test.shape[0])\nmin_all_test=min_all_test.reshape(min_all_test.shape[0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Dropout\nnum_epoch=100\nbatch_size=48\nlr=0.0001\noptimizer = keras.optimizers.Adam(learning_rate=lr)\nlstm_units=30\nmodel=Sequential()\nmodel.add(LSTM(units=lstm_units,input_shape=(x_train.shape[1],1)))\nmodel.add(Dense(units=20,activation='relu'))\nmodel.add(Dense(units=10,activation='relu'))\nmodel.add(Dense(units=1,activation='relu'))\nmodel.compile(optimizer=optimizer,loss='mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train,y_train,\n          batch_size=60,\n          epochs=100,\n         validation_data=(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_train=model.predict(x_train)\ny_predict_test=model.predict(x_test)\n\n#Flatenning data frame\ny_train_flatten=np.array(y_train).ravel()\ny_predict_train_flatten=np.array(y_predict_train).ravel()\n\ny_test_flatten=np.array(y_test).ravel()\ny_predict_test_flatten=np.array(y_predict_test).ravel()\n\ny_state_test_flatten=np.array(y_state_test).ravel()\ny_date_flatten=np.array(y_date).ravel()\n\nmax_all_train=np.array(max_all_train).ravel()\nmin_all_train=np.array(min_all_train).ravel()\n\nmax_all_tets=np.array(max_all_test).ravel()\nmin_all_tets=np.array(min_all_test).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_loss_train=pd.DataFrame(list(zip(y_train_flatten,y_predict_train_flatten,max_all_train,min_all_train)),\n               columns=['y_train','y_predicted','max_value','min_value'])\n\nprint(df_loss_train.shape)\ndf_loss_train['diff']=df_loss_train['max_value']-df_loss_train['min_value']\ndf_loss_train['y_train_unscaled']=(df_loss_train['y_train']*(df_loss_train['diff']))+df_loss_train['min_value']\ndf_loss_train['y_predicted_unscaled']=(df_loss_train['y_predicted']*(df_loss_train['diff']))+df_loss_train['min_value']\ndf_loss_train=pd.DataFrame(list(zip(y_train_flatten,y_predict_train_flatten,max_all_train,min_all_train)),\n               columns=['y_train','y_predicted','max_value','min_value'])\n\ndf_loss_train['diff']=df_loss_train['max_value']-df_loss_train['min_value']\ndf_loss_train['y_train_unscaled']=(df_loss_train['y_train']*(df_loss_train['diff']))+df_loss_train['min_value']\ndf_loss_train['y_predicted_unscaled']=(df_loss_train['y_predicted']*(df_loss_train['diff']))+df_loss_train['min_value']\n#df_loss_train.head(200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_loss_test=pd.DataFrame(list(zip(y_test_flatten,y_predict_test_flatten,max_all_test,min_all_test,y_state_test_flatten,y_date_flatten,y_test_unscaled)),\n               columns=['y_test','y_predicted','max_value','min_value','State','Date','y_test_unscaled'])\ndf_loss_test['diff']=df_loss_test['max_value']-df_loss_test['min_value']\ndf_loss_test['y_test_unscaled']=df_loss_test['y_test']*(df_loss_test['diff'])+df_loss_test['min_value']\ndf_loss_test['y_predicted_unscaled']=(df_loss_test['y_predicted']*(df_loss_test['diff']))+df_loss_test['min_value']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predictions from 26th May to June 3rd","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_loss_test.loc[df_loss_test['State']=='Andhra Pradesh'].sort_values(by=['State','Date'],ascending=[True,True]).head(100)\n#df_loss_test.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assumption 1: Going by demographics of India where age -demograpics are as following:-\n25 % - very young 0 - 18 \n45% - youth  \n30% - senior citizen (55+) \nHere we are assuming 50% young and youth will require hospitalization from the total number of predicted patients\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''print(population_india.columns)\n#population=\nprint(df_loss_test.columns)\n#Merging to obtain demographics data \ntest_predicted_data=pd.merge(df_loss_test,population_india[['Population','State / Union Territory']],how='left',left_on='State',right_on='State / Union Territory')\ntest_predicted_data['Young_and_Youth']=np.round((test_predicted_data['Population']*0.7),0)\ntest_predicted_data['Senior_Citizen']=np.round((test_predicted_data['Population']*0.3),0)\ntest_predicted_data['Hospitilization_required']=np.round((0.5*test_predicted_data['Young_and_Youth']),0)+test_predicted_data['Senior_Citizen']\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_loss_test['Hospitilization_required']=np.round((0.65*df_loss_test['y_predicted_unscaled']),0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_loss_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hospitilization_data=test_predicted_data.loc[:,['State','Date','y_test_unscaled','y_predicted_unscaled','Hospitilization_required']]\nhospitilization_data.columns=['State','Date','Observed_Patients','Predicted_Patients','Hospitilization_required']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hospitilization_data['Required_Beds']=hospitilization_data['Hospitilization_required']\nhospital_beds[\"Available Beds\"]=hospital_beds['NumPublicBeds_HMIS']+hospital_beds['NumRuralBeds_NHP18']+hospital_beds['NumUrbanBeds_NHP18']\nhospital_beds[\"Available_Corona_Beds\"]=np.round((0.05*hospital_beds[\"Available Beds\"]),0)\n\n#hospital_beds=hospital_beds.loc[:,['State/UT','Total_Beds']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merging data to get total no of hospital beds present in india \nhospital_beds['State/UT']=hospital_beds['State/UT'].replace(\"Andaman & Nicobar Islands\",\"Andaman and Nicobar Islands\")\nhospital_beds['State/UT']=hospital_beds['State/UT'].replace(\"Dadra and Nagar Haveli\",\"Dadra and Nagar Haveli and Daman and Diu\")\nhospital_beds['State/UT']=hospital_beds['State/UT'].replace(\"Jammu & Kashmir\",\"Jammu and Kashmir\")\n\nall_hospitliazation=pd.merge(hospitilization_data,hospital_beds[['State/UT','Available Beds','Available_Corona_Beds']],left_on='State',right_on='State/UT',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_hospitliazation.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets plot gap between required and total beds for each state for month of June","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_hospitliazation['Gap_in_required_beds']=all_hospitliazation['Required_Beds']-all_hospitliazation['Available_Corona_Beds']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_deaths=all_hospitliazation.loc[all_hospitliazation['Date']=='2020-06-01',:].sort_values(by='Gap_in_required_beds',ascending=False)[0:35]\nimport seaborn as sns\nplt.figure(figsize= (10,8))\nplt.xticks(fontsize = 10)\nplt.yticks(fontsize = 10)\nplt.xlabel(\"Gap_in_required_beds\",fontsize = 30)\nplt.ylabel('State',fontsize = 30)\nplt.title(\"Top 5 states having most Gap in required_beds for 1st June 2020\" , fontsize = 30)\nax = sns.barplot(x = top_deaths.Gap_in_required_beds, y = top_deaths.State)\nfor i, (value, name) in enumerate(zip(top_deaths.Gap_in_required_beds,top_deaths.State)):\n    ax.text(value, i-.05, f'{value:,.0f}',  size=10, ha='left',  va='center')\nax.set(xlabel='Total cases', ylabel='State/UT')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_deaths=all_hospitliazation.loc[all_hospitliazation['Date']=='2020-06-02',:].sort_values(by='Gap_in_required_beds',ascending=False)[0:35]\nimport seaborn as sns\nplt.figure(figsize= (10,8))\nplt.xticks(fontsize = 10)\nplt.yticks(fontsize = 10)\nplt.xlabel(\"Gap_in_required_beds\",fontsize = 30)\nplt.ylabel('State',fontsize = 30)\nplt.title(\"Top 5 states having most Gap in required_beds for 2nd June 2020\" , fontsize = 30)\nax = sns.barplot(x = top_deaths.Gap_in_required_beds, y = top_deaths.State)\nfor i, (value, name) in enumerate(zip(top_deaths.Gap_in_required_beds,top_deaths.State)):\n    ax.text(value, i-.05, f'{value:,.0f}',  size=10, ha='left',  va='center')\nax.set(xlabel='Total cases', ylabel='State/UT')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_deaths=all_hospitliazation.loc[all_hospitliazation['Date']=='2020-06-03',:].sort_values(by='Gap_in_required_beds',ascending=False)[0:35]\nimport seaborn as sns\nplt.figure(figsize= (10,8))\nplt.xticks(fontsize = 10)\nplt.yticks(fontsize = 10)\nplt.xlabel(\"Gap_in_required_beds\",fontsize = 30)\nplt.ylabel('State',fontsize = 30)\nplt.title(\"Top 5 states having most Gap in required_beds for 3rd June 2020\" , fontsize = 30)\nax = sns.barplot(x = top_deaths.Gap_in_required_beds, y = top_deaths.State)\nfor i, (value, name) in enumerate(zip(top_deaths.Gap_in_required_beds,top_deaths.State)):\n    ax.text(value, i-.05, f'{value:,.0f}',  size=10, ha='left',  va='center')\nax.set(xlabel='Total cases', ylabel='State/UT')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}